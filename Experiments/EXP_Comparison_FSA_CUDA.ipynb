{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Federated Survival Analysis Simulation with Growing Data and Trust Comparisons\n",
    "\n",
    "This notebook simulates three scenarios in federated survival analysis:\n",
    "\n",
    "1. **Our Model:** Our custom trust mechanism with custom clustering and a trust update based on the improvement in the c-index. Local training uses a growing fraction of each node’s data over rounds.\n",
    "2. **Baseline (FLTrust):** A baseline trust mechanism inspired by FLTrust where (a) clustering is done using standard KMeans on the feature completeness vectors and (b) trust updates use a lower learning rate.\n",
    "3. **No Trust:** Plain federated averaging (FedAvg) without any trust mechanism (all nodes weighted equally).\n",
    "\n",
    "At every round we record the number of nodes whose c-index improved compared to the previous round. Finally, a table is generated and t-tests are performed to statistically compare the three approaches.\n",
    "\n",
    "Before running, please select a GPU runtime in Google Colab (Runtime → Change runtime type → Hardware accelerator: GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install CuPy for your Colab GPU runtime (uncomment if necessary):\n",
    "# !pip install cupy-cuda11x\n",
    "\n",
    "!pip install lifelines\n",
    "!pip install scikit-learn\n",
    "\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Try to import cupy. If not available, fallback to numpy.\n",
    "try:\n",
    "    import cupy as cp\n",
    "except ImportError:\n",
    "    cp = np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Synthetic Data Generation (HDF5 Files)\n",
    "##############################################\n",
    "\n",
    "# def generate_synthetic_data(\n",
    "#     num_datasets=10,\n",
    "#     num_rows_range=(100, 200),\n",
    "#     total_features=50,\n",
    "#     common_features=10,\n",
    "#     censoring_percentage=0.30,\n",
    "#     missing_value_fraction=0.1,\n",
    "#     missingness_range=(0, 0.99),\n",
    "#     feature_null_fraction=0.3,\n",
    "#     output_dir=\"datasets_h5\",\n",
    "#     filename=None  # Optional specific filename\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Generate synthetic survival analysis datasets with metadata stored in HDF5 format.\n",
    "#     Each dataset file contains:\n",
    "#       - A data matrix (features, time, death)\n",
    "#       - A metadata group with:\n",
    "#           * node_features: the list of features available at the center.\n",
    "#           * binary_feature_vector: a binary vector of feature presence.\n",
    "#           * feature_vector: the real-valued feature completeness vector.\n",
    "#           * null_counts: number of null rows per available feature.\n",
    "#           * Attributes: date_created, num_rows, num_features, num_censored.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     all_features = [f\"feat_{i}\" for i in range(total_features)]\n",
    "#     all_features_path = os.path.join(output_dir, \"all_features.txt\")\n",
    "#     with open(all_features_path, \"w\") as f:\n",
    "#         f.write(\"\\n\".join(all_features))\n",
    "#     print(f\"Global feature set saved to {all_features_path}\")\n",
    "\n",
    "#     for i in range(num_datasets):\n",
    "#         num_rows = random.randint(*num_rows_range)\n",
    "#         lower_bound = int(common_features * 0.1)\n",
    "#         upper_bound = int(common_features * 0.5)\n",
    "#         num_features = common_features + random.randint(lower_bound, upper_bound)\n",
    "#         f_i = sorted(random.sample(all_features, num_features))\n",
    "#         values = np.random.default_rng().normal(loc=0, scale=1/num_features, size=(num_rows, num_features))\n",
    "#         df = pd.DataFrame(values, columns=f_i)\n",
    "\n",
    "#         null_counts = {}\n",
    "#         num_null_features = int(feature_null_fraction * len(f_i))\n",
    "#         features_with_nulls = random.sample(f_i, num_null_features)\n",
    "#         for feature in features_with_nulls:\n",
    "#             null_percentage = random.uniform(*missingness_range)\n",
    "#             num_missing = int(null_percentage * num_rows)\n",
    "#             null_counts[feature] = num_missing\n",
    "#             missing_indices = np.random.choice(num_rows, num_missing, replace=False)\n",
    "#             df.loc[missing_indices, feature] = np.nan\n",
    "\n",
    "#         gamma = [random.uniform(-1, 1) for _ in f_i]\n",
    "#         temp = df[f_i].fillna(0).values @ gamma\n",
    "#         time_exponential = np.random.default_rng().exponential(scale=5 * np.exp(temp))\n",
    "#         time = time_exponential.astype(int) + 1\n",
    "\n",
    "#         d_nums = int(num_rows * (1 - censoring_percentage))\n",
    "#         d_arr = np.array([1] * d_nums + [0] * (num_rows - d_nums))\n",
    "#         np.random.shuffle(d_arr)\n",
    "#         for j in range(len(d_arr)):\n",
    "#             if d_arr[j] == 0:\n",
    "#                 time[j] = random.randint(1, time[j])\n",
    "#         df[\"time\"] = time\n",
    "#         df[\"death\"] = d_arr\n",
    "\n",
    "#         binary_feature_vector = [1 if feature in f_i else 0 for feature in all_features]\n",
    "#         feature_vector = []\n",
    "#         for feature in all_features:\n",
    "#             if feature in f_i:\n",
    "#                 num_non_null = df[feature].notnull().sum()\n",
    "#                 feature_value = num_non_null / num_rows\n",
    "#             else:\n",
    "#                 feature_value = 0\n",
    "#             feature_vector.append(feature_value)\n",
    "#         max_value = max(feature_vector) if feature_vector else 1\n",
    "#         feature_vector = [value / max_value for value in feature_vector]\n",
    "\n",
    "#         if filename:\n",
    "#             dataset_path = os.path.join(output_dir, filename)\n",
    "#         else:\n",
    "#             dataset_path = os.path.join(output_dir, f\"dataset_node_{i}.h5\")\n",
    "#         with h5py.File(dataset_path, \"w\") as h5f:\n",
    "#             h5f.create_dataset(\"data\", data=df.values, compression=\"gzip\")\n",
    "#             metadata_group = h5f.create_group(\"metadata\")\n",
    "#             metadata_group.create_dataset(\"node_features\", data=np.array(f_i, dtype=\"S10\"))\n",
    "#             metadata_group.create_dataset(\"gamma_values\", data=np.array(gamma))\n",
    "#             metadata_group.create_dataset(\"binary_feature_vector\", data=binary_feature_vector)\n",
    "#             metadata_group.create_dataset(\"feature_vector\", data=feature_vector)\n",
    "#             metadata_group.create_dataset(\"null_counts\", data=[null_counts.get(f, 0) for f in f_i])\n",
    "#             metadata_group.attrs[\"date_created\"] = str(datetime.datetime.now())\n",
    "#             metadata_group.attrs[\"num_rows\"] = num_rows\n",
    "#             metadata_group.attrs[\"num_features\"] = num_features\n",
    "#             metadata_group.attrs[\"num_censored\"] = num_rows - d_nums\n",
    "\n",
    "#         print(f\"Dataset {i + 1}/{num_datasets} saved to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Helper Functions for Federated Survival Analysis\n",
    "##############################################\n",
    "\n",
    "def load_node_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load the dataset and metadata from an HDF5 file.\n",
    "    Returns:\n",
    "      - df: DataFrame with data (columns: features..., time, death)\n",
    "      - node_features: list of features available at this node\n",
    "      - feature_vector: real-valued vector (length global_dim) indicating feature completeness\n",
    "      - num_rows: number of rows (sample size) at this node (from metadata attribute)\n",
    "    \"\"\"\n",
    "    with h5py.File(filepath, \"r\") as h5f:\n",
    "        data = h5f[\"data\"][:]\n",
    "        metadata = h5f[\"metadata\"]\n",
    "        node_features = [s.decode(\"utf-8\") for s in metadata[\"node_features\"][:]]\n",
    "        feature_vector = np.array(metadata[\"feature_vector\"][:])\n",
    "        num_rows = int(metadata.attrs[\"num_rows\"])\n",
    "        col_names = node_features + [\"time\", \"death\"]\n",
    "        df = pd.DataFrame(data, columns=col_names)\n",
    "    return df, node_features, feature_vector, num_rows\n",
    "\n",
    "def get_global_coef_from_model(cox_model, node_features, global_dim=50):\n",
    "    \"\"\"\n",
    "    Embed the local CoxPH model coefficients into a global vector of size global_dim.\n",
    "    \"\"\"\n",
    "    coef_full = np.zeros(global_dim)\n",
    "    for feat in node_features:\n",
    "        try:\n",
    "            idx = int(feat.split(\"_\")[1])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if feat in cox_model.params_.index:\n",
    "            coef_full[idx] = cox_model.params_[feat]\n",
    "    return coef_full\n",
    "\n",
    "def evaluate_cindex(coef, node_df, node_features, global_dim=50):\n",
    "    \"\"\"\n",
    "    Evaluate the concordance index (c-index) on a node’s dataset given the coefficient vector.\n",
    "    Only the coefficients corresponding to the node’s features are used.\n",
    "    \"\"\"\n",
    "    node_df = node_df.fillna(0)\n",
    "    risk_scores = np.zeros(len(node_df))\n",
    "    for feat in node_features:\n",
    "        try:\n",
    "            idx = int(feat.split(\"_\")[1])\n",
    "        except:\n",
    "            continue\n",
    "        if feat in node_df.columns:\n",
    "            risk_scores += node_df[feat].astype(float).values * coef[idx]\n",
    "    times = node_df[\"time\"].astype(float).values\n",
    "    events = node_df[\"death\"].astype(int).values\n",
    "    return concordance_index(times, -risk_scores, events)\n",
    "\n",
    "def generate_noise(node, round_num, T_honest, T_ramp, epsilon_max):\n",
    "    \"\"\"\n",
    "    Generate noise for a node (if it is designated as noisy).\n",
    "    \"\"\"\n",
    "    if round_num < T_honest:\n",
    "        return np.zeros_like(node['z'])\n",
    "    cycle_phase = (round_num - T_honest) % (T_honest + T_ramp)\n",
    "    if cycle_phase < T_ramp:\n",
    "        alpha_i = min(cycle_phase / T_ramp, epsilon_max)\n",
    "        return alpha_i * node['z']\n",
    "    else:\n",
    "        return np.zeros_like(node['z'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predicted_risk",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# New Helper: Compute Predicted Risk for Clustering\n",
    "##############################################\n",
    "\n",
    "def compute_predicted_risk(node):\n",
    "    \"\"\"\n",
    "    Compute a summary risk for a node using its local model coefficients.\n",
    "    For each row in the node's data (only the available features), compute the dot product with the corresponding local coefficients, and return the average.\n",
    "    \"\"\"\n",
    "    df_local = node['data'][node['features']].fillna(0)\n",
    "    coefs = []\n",
    "    for feat in node['features']:\n",
    "        try:\n",
    "            idx = int(feat.split('_')[1])\n",
    "            coefs.append(node['local_coef'][idx])\n",
    "        except:\n",
    "            coefs.append(0)\n",
    "    coefs = np.array(coefs)\n",
    "    risk_values = df_local.dot(coefs)\n",
    "    return np.mean(risk_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom_clustering",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# New Clustering Function (Custom Clustering)\n",
    "##############################################\n",
    "\n",
    "def custom_clustering(nodes, n_clusters, lambda_clust, max_iter=10):\n",
    "    \"\"\"\n",
    "    Custom clustering that minimizes the objective:\n",
    "      \\(\\sum_{i=1}^{c} \\sum_{j \\in C_i} \\|B_j - \\mu_i\\|_2 + \\lambda_{\\text{clust}} \\cdot |r_j - \\bar{r}_i|\\),\n",
    "    where \\(B_j\\) is the feature vector of node j, \\(r_j\\) is its predicted risk, and \\(\\bar{r}_i\\) is the average risk in cluster i.\n",
    "    Returns a dictionary mapping cluster labels to lists of node indices and updates each node with its cluster assignment.\n",
    "    \"\"\"\n",
    "    # Initialize cluster assignment using KMeans on feature vectors\n",
    "    X = np.array([node['feature_vector'] for node in nodes])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    for i, node in enumerate(nodes):\n",
    "        node['cluster'] = labels[i]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        centroids = {}\n",
    "        avg_risks = {}\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for i, node in enumerate(nodes):\n",
    "            clusters[node['cluster']].append(i)\n",
    "        for i in range(n_clusters):\n",
    "            if len(clusters[i]) > 0:\n",
    "                B = np.array([nodes[j]['feature_vector'] for j in clusters[i]])\n",
    "                centroids[i] = np.mean(B, axis=0)\n",
    "                risks = [compute_predicted_risk(nodes[j]) for j in clusters[i]]\n",
    "                avg_risks[i] = np.mean(risks)\n",
    "            else:\n",
    "                centroids[i] = np.zeros_like(nodes[0]['feature_vector'])\n",
    "                avg_risks[i] = 0\n",
    "\n",
    "        changed = False\n",
    "        for i, node in enumerate(nodes):\n",
    "            B_j = np.array(node['feature_vector'])\n",
    "            risk_j = compute_predicted_risk(node)\n",
    "            best_cost = None\n",
    "            best_cluster = None\n",
    "            for c in range(n_clusters):\n",
    "                cost = np.linalg.norm(B_j - centroids[c]) + lambda_clust * abs(risk_j - avg_risks[c])\n",
    "                if best_cost is None or cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_cluster = c\n",
    "            if best_cluster != node['cluster']:\n",
    "                changed = True\n",
    "                node['cluster'] = best_cluster\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    cluster_assignment = {i: [] for i in range(n_clusters)}\n",
    "    for i, node in enumerate(nodes):\n",
    "        cluster_assignment[node['cluster']].append(i)\n",
    "    return cluster_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_fedavg",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Cluster-wise Federated Averaging\n",
    "##############################################\n",
    "\n",
    "def cluster_fedavg(nodes, cluster_assignment):\n",
    "    \"\"\"\n",
    "    For each cluster (given by cluster_assignment, a dict mapping cluster label -> list of node indices),\n",
    "    compute the weighted average of the local coefficients (node['local_coef']) weighted by node['num_rows'].\n",
    "    Update each node's global_coef with its cluster’s aggregated beta.\n",
    "    \"\"\"\n",
    "    for clabel, node_ids in cluster_assignment.items():\n",
    "        total_samples = sum(nodes[i]['num_rows'] for i in node_ids)\n",
    "        if total_samples == 0:\n",
    "            continue\n",
    "        weighted_sum = np.zeros_like(nodes[node_ids[0]]['local_coef'])\n",
    "        for i in node_ids:\n",
    "            weighted_sum += nodes[i]['local_coef'] * nodes[i]['num_rows']\n",
    "        cluster_global_coef = weighted_sum / total_samples\n",
    "        for i in node_ids:\n",
    "            nodes[i]['global_coef'] = cluster_global_coef.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Main Simulation with Growing Data and Three Scenarios\n",
    "##############################################\n",
    "\n",
    "def simulate_scenario(scenario, epsilon_max, alpha, lambda_clust, n_clusters, monte_carlo_runs, max_rounds):\n",
    "    \"\"\"\n",
    "    Simulate the federated learning process for one scenario.\n",
    "    scenario: 'our', 'baseline', or 'no_trust'\n",
    "    For each round, use a fraction of each node's data that grows linearly over rounds.\n",
    "    Returns an array of length max_rounds where each entry is the number of nodes with improvement in c-index compared to the previous round (averaged over Monte Carlo runs).\n",
    "    \"\"\"\n",
    "    num_nodes = 10\n",
    "    global_dim = 50\n",
    "    T_honest = 10\n",
    "    T_ramp = 5\n",
    "\n",
    "    # Load nodes from datasets\n",
    "    node_files = sorted(glob.glob(os.path.join(\"datasets_h5\", \"dataset_node_*.h5\")))\n",
    "    nodes = []\n",
    "    noisy_node_ids = {0, 4, 9}  \n",
    "    for i, filepath in enumerate(node_files):\n",
    "        df, node_features, feature_vector, num_rows = load_node_dataset(filepath)\n",
    "        if i in noisy_node_ids:\n",
    "            z = np.random.uniform(-1, 1, size=len(node_features))\n",
    "        else:\n",
    "            z = np.zeros(len(node_features))\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'data': df,\n",
    "            'features': node_features,\n",
    "            'feature_vector': feature_vector,\n",
    "            'num_rows': num_rows,\n",
    "            'local_coef': np.zeros(global_dim),\n",
    "            'global_coef': np.zeros(global_dim),\n",
    "            'trust': np.ones(num_nodes),\n",
    "            'is_noisy': i in noisy_node_ids,\n",
    "            'noise': np.zeros(len(node_features)),\n",
    "            'z': z\n",
    "        })\n",
    "\n",
    "    # Prepare to record improvement counts\n",
    "    improvements_all = np.zeros(max_rounds)\n",
    "\n",
    "    for mc in range(monte_carlo_runs):\n",
    "        # Reinitialize nodes for this Monte Carlo run\n",
    "        for node in nodes:\n",
    "            node['trust'] = np.ones(num_nodes)\n",
    "            node['global_coef'] = np.zeros(global_dim)\n",
    "            if node['is_noisy']:\n",
    "                node['noise'] = np.zeros(len(node['features']))\n",
    "            # For each node, initialize previous c-index using all data fraction of round0\n",
    "            # Here, we run a quick local training on a very small fraction\n",
    "            fraction = 0.01\n",
    "            df_local = node['data'].iloc[:max(1, int(len(node['data']) * fraction))].fillna(0)\n",
    "            try:\n",
    "                cox_model = CoxPHFitter()\n",
    "                cox_model.fit(df_local, duration_col=\"time\", event_col=\"death\", show_progress=False)\n",
    "                prev_cindex = evaluate_cindex(get_global_coef_from_model(cox_model, node['features'], global_dim), df_local, node['features'], global_dim)\n",
    "            except Exception as e:\n",
    "                prev_cindex = 0\n",
    "            node['prev_cindex'] = prev_cindex\n",
    "\n",
    "        improvements = []\n",
    "\n",
    "        for round_num in range(max_rounds):\n",
    "            fraction = min(1.0, (round_num + 1) / max_rounds)\n",
    "\n",
    "            # --- Local Training ---\n",
    "            for node in nodes:\n",
    "                df_local = node['data'].iloc[:max(1, int(len(node['data']) * fraction))].copy()\n",
    "                df_local = df_local.fillna(0)\n",
    "                try:\n",
    "                    cox_model = CoxPHFitter()\n",
    "                    cox_model.fit(df_local, duration_col=\"time\", event_col=\"death\", show_progress=False)\n",
    "                    local_coef = get_global_coef_from_model(cox_model, node['features'], global_dim)\n",
    "                except Exception as e:\n",
    "                    local_coef = node['local_coef']\n",
    "                if node['is_noisy']:\n",
    "                    delta_noise = generate_noise(node, round_num, T_honest, T_ramp, epsilon_max)\n",
    "                    node['noise'] += delta_noise\n",
    "                    for idx_local, feat in enumerate(node['features']):\n",
    "                        try:\n",
    "                            global_idx = int(feat.split(\"_\")[1])\n",
    "                        except:\n",
    "                            continue\n",
    "                        local_coef[global_idx] += node['noise'][idx_local]\n",
    "                node['local_coef'] = local_coef.copy()\n",
    "                new_cindex = evaluate_cindex(node['local_coef'], df_local, node['features'], global_dim)\n",
    "                node['new_cindex'] = new_cindex\n",
    "\n",
    "            # --- Trust and Aggregation based on Scenario ---\n",
    "            if scenario == \"our\":\n",
    "                cluster_assignment = custom_clustering(nodes, n_clusters, lambda_clust, max_iter=10)\n",
    "                # Trust update as in our simulation\n",
    "                for clabel, node_ids in cluster_assignment.items():\n",
    "                    for i in node_ids:\n",
    "                        # For selected nodes (simulate selection as in our simulation):\n",
    "                        for peer_id in node_ids:\n",
    "                            if peer_id == i:\n",
    "                                continue\n",
    "                            peer = nodes[peer_id]\n",
    "                            improvement = nodes[i]['new_cindex'] - nodes[i]['prev_cindex']\n",
    "                            nodes[i]['trust'][peer_id] = max(0, min(nodes[i]['trust'][peer_id] + alpha * improvement, 1))\n",
    "                cluster_fedavg(nodes, cluster_assignment)\n",
    "\n",
    "            elif scenario == \"baseline\":\n",
    "                # Simulate FLTrust baseline: use standard KMeans clustering (without risk adjustment)\n",
    "                X = np.array([node['feature_vector'] for node in nodes])\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "                labels = kmeans.labels_\n",
    "                for i, node in enumerate(nodes):\n",
    "                    node['cluster'] = labels[i]\n",
    "                # Baseline trust update with a lower alpha (e.g., 0.05) and no risk difference term\n",
    "                alpha_baseline = 0.05\n",
    "                clusters = {i: [] for i in range(n_clusters)}\n",
    "                for i, node in enumerate(nodes):\n",
    "                    clusters[node['cluster']].append(i)\n",
    "                for clabel, node_ids in clusters.items():\n",
    "                    for i in node_ids:\n",
    "                        for peer_id in node_ids:\n",
    "                            if peer_id == i:\n",
    "                                continue\n",
    "                            improvement = nodes[i]['new_cindex'] - nodes[i]['prev_cindex']\n",
    "                            nodes[i]['trust'][peer_id] = max(0, min(nodes[i]['trust'][peer_id] + alpha_baseline * improvement, 1))\n",
    "                # Use standard KMeans centroid for aggregation\n",
    "                # For simplicity, average all local coefficients in each cluster\n",
    "                cluster_fedavg(nodes, clusters)\n",
    "\n",
    "            elif scenario == \"no_trust\":\n",
    "                # No trust update: simply average all local models equally\n",
    "                for node in nodes:\n",
    "                    node['trust'] = np.ones(num_nodes)\n",
    "                # Federated averaging without clustering\n",
    "                global_coef = np.mean([node['local_coef'] for node in nodes], axis=0)\n",
    "                for node in nodes:\n",
    "                    node['global_coef'] = global_coef.copy()\n",
    "\n",
    "            # --- Count Improvement: number of nodes with new_cindex > prev_cindex ---\n",
    "            improvement_count = sum(1 for node in nodes if node['new_cindex'] > node['prev_cindex'])\n",
    "            improvements.append(improvement_count)\n",
    "\n",
    "            # Update prev_cindex for next round\n",
    "            for node in nodes:\n",
    "                node['prev_cindex'] = node['new_cindex']\n",
    "\n",
    "        improvements_all += np.array(improvements)\n",
    "\n",
    "    improvements_all /= monte_carlo_runs\n",
    "    return improvements_all\n",
    "\n",
    "# Run simulations for the three scenarios\n",
    "max_rounds = 100\n",
    "monte_carlo_runs = 1  # For illustration; increase for statistical significance\n",
    "our_results = simulate_scenario(\"our\", epsilon_max=0.2, alpha=0.1, lambda_clust=0.6, n_clusters=4, monte_carlo_runs=monte_carlo_runs, max_rounds=max_rounds)\n",
    "baseline_results = simulate_scenario(\"baseline\", epsilon_max=0.2, alpha=0.1, lambda_clust=0.6, n_clusters=4, monte_carlo_runs=monte_carlo_runs, max_rounds=max_rounds)\n",
    "no_trust_results = simulate_scenario(\"no_trust\", epsilon_max=0.2, alpha=0.1, lambda_clust=0.6, n_clusters=4, monte_carlo_runs=monte_carlo_runs, max_rounds=max_rounds)\n",
    "\n",
    "# Create a DataFrame table comparing the three scenarios (improvement counts per round)\n",
    "df_comparison = pd.DataFrame({\n",
    "    'Round': np.arange(1, max_rounds + 1),\n",
    "    'Our_Model': our_results,\n",
    "    'FLTrust_Baseline': baseline_results,\n",
    "    'No_Trust': no_trust_results\n",
    "})\n",
    "print(df_comparison.head())\n",
    "\n",
    "# Perform t-tests comparing Our_Model vs FLTrust_Baseline and Our_Model vs No_Trust\n",
    "t_stat1, p_val1 = ttest_ind(our_results, baseline_results)\n",
    "t_stat2, p_val2 = ttest_ind(our_results, no_trust_results)\n",
    "print(\"\\nStatistical Comparison (T-Test):\")\n",
    "print(\"Our_Model vs FLTrust_Baseline: t-statistic = {:.3f}, p-value = {:.3f}\".format(t_stat1, p_val1))\n",
    "print(\"Our_Model vs No_Trust: t-statistic = {:.3f}, p-value = {:.3f}\".format(t_stat2, p_val2))\n",
    "\n",
    "# Save the comparison table as a LaTeX table\n",
    "latex_table = df_comparison.to_latex(index=False)\n",
    "with open(\"comparison_table.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "print(\"LaTeX comparison table saved to comparison_table.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noise_experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Experiment: Noise Injection Magnitude over Time\n",
    "##############################################\n",
    "\n",
    "def test_noise_injection(T_honest, T_ramp, epsilon_max_values, rounds=200, d=1):\n",
    "    \"\"\"\n",
    "    For a dummy noisy node with a noise direction vector z (of dimension d),\n",
    "    compute the instantaneous noise injection magnitude (n_t) for each round t,\n",
    "    using the generate_noise function. Returns a dictionary mapping each epsilon_max\n",
    "    value to the list of noise magnitudes over time.\n",
    "    \"\"\"\n",
    "    noise_curves = {}\n",
    "    rounds_list = np.arange(rounds)\n",
    "    for eps in epsilon_max_values:\n",
    "        noise_vals = []\n",
    "        dummy_node = {'z': np.ones(d)}  # simple vector of ones\n",
    "        for t in rounds_list:\n",
    "            noise = generate_noise(dummy_node, t, T_honest, T_ramp, eps)\n",
    "            magnitude = np.linalg.norm(noise)\n",
    "            noise_vals.append(magnitude)\n",
    "        noise_curves[eps] = noise_vals\n",
    "    return rounds_list, noise_curves\n",
    "\n",
    "# Set parameters for the noise injection experiment\n",
    "T_honest_test = 20\n",
    "T_ramp_test = 10\n",
    "epsilon_max_values = [0.05, 0.1, 0.2, 0.4, 0.8]\n",
    "rounds_test = 200\n",
    "\n",
    "rounds_list, noise_curves = test_noise_injection(T_honest_test, T_ramp_test, epsilon_max_values, rounds=rounds_test, d=1)\n",
    "\n",
    "output_dir = \"noise_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for eps, curve in noise_curves.items():\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(rounds_list, curve, label=f'epsilon_max = {eps}')\n",
    "    plt.xlabel('Round (t)')\n",
    "    plt.ylabel('Noise magnitude n_t')\n",
    "    plt.title('Noise magnitude over time (T_honest = {}, T_ramp = {})'.format(T_honest_test, T_ramp_test))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    filename = os.path.join(output_dir, f\"noise_plot_epsilon_{eps}.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved noise plot for epsilon_max = {eps} to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
