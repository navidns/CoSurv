{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c2c893",
   "metadata": {},
   "source": [
    "# Federated Survival Analysis Simulation with Custom Clustering and Growing Data\n",
    "\n",
    "This notebook implements our federated survival analysis simulation with GPU acceleration (using CuPy) and a custom clustering step that minimizes an objective combining Euclidean distance between feature completeness vectors and the difference in predicted risk. In addition, the local training is modified so that at each round only a fraction of the data is used—and that fraction grows over the rounds (simulating a warm-up or incremental data availability scenario).\n",
    "\n",
    "Before running the notebook in Google Colab, please select a GPU runtime (Runtime → Change runtime type → Hardware accelerator: GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install CuPy for your Colab GPU runtime (uncomment if necessary):\n",
    "# !pip install cupy-cuda11x\n",
    "\n",
    "!pip install lifelines\n",
    "!pip install scikit-learn\n",
    "\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import datetime\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Try to import cupy. If not available, fallback to numpy.\n",
    "try:\n",
    "    import cupy as cp\n",
    "except ImportError:\n",
    "    cp = np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aafe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Synthetic Data Generation (HDF5 Files)\n",
    "##############################################\n",
    "\n",
    "def generate_synthetic_data(\n",
    "    num_datasets=10,\n",
    "    num_rows_range=(100, 200),\n",
    "    total_features=50,\n",
    "    common_features=10,\n",
    "    censoring_percentage=0.30,\n",
    "    missing_value_fraction=0.1,\n",
    "    missingness_range=(0, 0.99),\n",
    "    feature_null_fraction=0.3,\n",
    "    output_dir=\"datasets_h5\",\n",
    "    filename=None  # Optional specific filename\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic survival analysis datasets with metadata stored in HDF5 format.\n",
    "    Each dataset file contains:\n",
    "      - A data matrix (features, time, death)\n",
    "      - A metadata group with:\n",
    "          * node_features: the list of features available at the center.\n",
    "          * binary_feature_vector: a binary vector of feature presence.\n",
    "          * feature_vector: the real-valued feature completeness vector.\n",
    "          * null_counts: number of null rows per available feature.\n",
    "          * Attributes: date_created, num_rows, num_features, num_censored.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define the global feature set T\n",
    "    all_features = [f\"feat_{i}\" for i in range(total_features)]\n",
    "    all_features_path = os.path.join(output_dir, \"all_features.txt\")\n",
    "    with open(all_features_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(all_features))\n",
    "    print(f\"Global feature set saved to {all_features_path}\")\n",
    "\n",
    "    for i in range(num_datasets):\n",
    "        num_rows = random.randint(*num_rows_range)\n",
    "        # Define node-specific feature set F_i\n",
    "        lower_bound = int(common_features * 0.1)\n",
    "        upper_bound = int(common_features * 0.5)\n",
    "        num_features = common_features + random.randint(lower_bound, upper_bound)\n",
    "        # Randomly sample F_i from T\n",
    "        f_i = sorted(random.sample(all_features, num_features))\n",
    "        # Generate random values for each feature in F_i\n",
    "        values = np.random.default_rng().normal(\n",
    "            loc=0, scale=1/num_features, size=(num_rows, num_features)\n",
    "        )\n",
    "        df = pd.DataFrame(values, columns=f_i)\n",
    "\n",
    "        # Introduce null values\n",
    "        null_counts = {}\n",
    "        num_null_features = int(feature_null_fraction * len(f_i))\n",
    "        features_with_nulls = random.sample(f_i, num_null_features)\n",
    "        for feature in features_with_nulls:\n",
    "            null_percentage = random.uniform(*missingness_range)\n",
    "            num_missing = int(null_percentage * num_rows)\n",
    "            null_counts[feature] = num_missing\n",
    "            missing_indices = np.random.choice(num_rows, num_missing, replace=False)\n",
    "            df.loc[missing_indices, feature] = np.nan\n",
    "\n",
    "        # Generate gamma values for survival time\n",
    "        gamma = [random.uniform(-1, 1) for _ in f_i]\n",
    "        temp = df[f_i].fillna(0).values @ gamma\n",
    "        time_exponential = np.random.default_rng().exponential(scale=5 * np.exp(temp))\n",
    "        time = time_exponential.astype(int) + 1\n",
    "\n",
    "        # Introduce censoring\n",
    "        d_nums = int(num_rows * (1 - censoring_percentage))\n",
    "        d_arr = np.array([1] * d_nums + [0] * (num_rows - d_nums))\n",
    "        np.random.shuffle(d_arr)\n",
    "        for j in range(len(d_arr)):\n",
    "            if d_arr[j] == 0:\n",
    "                time[j] = random.randint(1, time[j])\n",
    "        df[\"time\"] = time\n",
    "        df[\"death\"] = d_arr\n",
    "\n",
    "        # Generate Binary Feature Vector and Feature Completeness Vector\n",
    "        binary_feature_vector = [1 if feature in f_i else 0 for feature in all_features]\n",
    "        feature_vector = []\n",
    "        for feature in all_features:\n",
    "            if feature in f_i:\n",
    "                num_non_null = df[feature].notnull().sum()\n",
    "                feature_value = num_non_null / num_rows\n",
    "            else:\n",
    "                feature_value = 0\n",
    "            feature_vector.append(feature_value)\n",
    "        # Max-normalize the feature vector\n",
    "        max_value = max(feature_vector) if feature_vector else 1\n",
    "        feature_vector = [value / max_value for value in feature_vector]\n",
    "\n",
    "        # Save dataset and metadata to HDF5\n",
    "        if filename:\n",
    "            dataset_path = os.path.join(output_dir, filename)\n",
    "        else:\n",
    "            dataset_path = os.path.join(output_dir, f\"dataset_node_{i}.h5\")\n",
    "        with h5py.File(dataset_path, \"w\") as h5f:\n",
    "            h5f.create_dataset(\"data\", data=df.values, compression=\"gzip\")\n",
    "            metadata_group = h5f.create_group(\"metadata\")\n",
    "            metadata_group.create_dataset(\"node_features\", data=np.array(f_i, dtype=\"S10\"))\n",
    "            metadata_group.create_dataset(\"gamma_values\", data=np.array(gamma))\n",
    "            metadata_group.create_dataset(\"binary_feature_vector\", data=binary_feature_vector)\n",
    "            metadata_group.create_dataset(\"feature_vector\", data=feature_vector)\n",
    "            metadata_group.create_dataset(\"null_counts\", data=[null_counts.get(f, 0) for f in f_i])\n",
    "            metadata_group.attrs[\"date_created\"] = str(datetime.datetime.now())\n",
    "            metadata_group.attrs[\"num_rows\"] = num_rows\n",
    "            metadata_group.attrs[\"num_features\"] = num_features\n",
    "            metadata_group.attrs[\"num_censored\"] = num_rows - d_nums\n",
    "\n",
    "        print(f\"Dataset {i + 1}/{num_datasets} saved to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94823d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Helper Functions for Federated Survival Analysis\n",
    "##############################################\n",
    "\n",
    "def load_node_dataset(filepath):\n",
    "    \"\"\"\n",
    "    Load the dataset and metadata from an HDF5 file.\n",
    "    Returns:\n",
    "      - df: DataFrame with data (columns: features..., time, death)\n",
    "      - node_features: list of features available at this node\n",
    "      - feature_vector: real-valued vector (length global_dim) indicating feature completeness\n",
    "      - num_rows: number of rows (sample size) at this node (from metadata attribute)\n",
    "    \"\"\"\n",
    "    with h5py.File(filepath, \"r\") as h5f:\n",
    "        data = h5f[\"data\"][:]\n",
    "        metadata = h5f[\"metadata\"]\n",
    "        node_features = [s.decode(\"utf-8\") for s in metadata[\"node_features\"][:]]\n",
    "        feature_vector = np.array(metadata[\"feature_vector\"][:])\n",
    "        num_rows = int(metadata.attrs[\"num_rows\"])\n",
    "        col_names = node_features + [\"time\", \"death\"]\n",
    "        df = pd.DataFrame(data, columns=col_names)\n",
    "    return df, node_features, feature_vector, num_rows\n",
    "\n",
    "def get_global_coef_from_model(cox_model, node_features, global_dim=50):\n",
    "    \"\"\"\n",
    "    Embed the local CoxPH model coefficients into a global vector of size global_dim.\n",
    "    \"\"\"\n",
    "    coef_full = np.zeros(global_dim)\n",
    "    for feat in node_features:\n",
    "        try:\n",
    "            idx = int(feat.split(\"_\")[1])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if feat in cox_model.params_.index:\n",
    "            coef_full[idx] = cox_model.params_[feat]\n",
    "    return coef_full\n",
    "\n",
    "def evaluate_cindex(coef, node_df, node_features, global_dim=50):\n",
    "    \"\"\"\n",
    "    Evaluate the concordance index (c-index) on a node’s dataset given the coefficient vector.\n",
    "    Only the coefficients corresponding to the node’s features are used.\n",
    "    \"\"\"\n",
    "    node_df = node_df.fillna(0)\n",
    "    risk_scores = np.zeros(len(node_df))\n",
    "    for feat in node_features:\n",
    "        try:\n",
    "            idx = int(feat.split(\"_\")[1])\n",
    "        except:\n",
    "            continue\n",
    "        if feat in node_df.columns:\n",
    "            risk_scores += node_df[feat].astype(float).values * coef[idx]\n",
    "    times = node_df[\"time\"].astype(float).values\n",
    "    events = node_df[\"death\"].astype(int).values\n",
    "    return concordance_index(times, -risk_scores, events)\n",
    "\n",
    "def generate_noise(node, round_num, T_honest, T_ramp, epsilon_max):\n",
    "    \"\"\"\n",
    "    Generate noise for a node (if it is designated as noisy).\n",
    "    \"\"\"\n",
    "    if round_num < T_honest:\n",
    "        return np.zeros_like(node['z'])\n",
    "    cycle_phase = (round_num - T_honest) % (T_honest + T_ramp)\n",
    "    if cycle_phase < T_ramp:\n",
    "        alpha_i = min(cycle_phase / T_ramp, epsilon_max)\n",
    "        return alpha_i * node['z']\n",
    "    else:\n",
    "        return np.zeros_like(node['z'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f894e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# New Helper: Compute Predicted Risk for Clustering\n",
    "##############################################\n",
    "\n",
    "def compute_predicted_risk(node):\n",
    "    \"\"\"\n",
    "    Compute a summary risk for a node using its local model coefficients.\n",
    "    For each row in the node's data (only the available features), compute the dot product with the corresponding local coefficients, and return the average.\n",
    "    \"\"\"\n",
    "    df_local = node['data'][node['features']].fillna(0)\n",
    "    coefs = []\n",
    "    for feat in node['features']:\n",
    "        try:\n",
    "            idx = int(feat.split('_')[1])\n",
    "            coefs.append(node['local_coef'][idx])\n",
    "        except:\n",
    "            coefs.append(0)\n",
    "    coefs = np.array(coefs)\n",
    "    risk_values = df_local.dot(coefs)\n",
    "    return np.mean(risk_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# New Clustering Function (Custom Clustering)\n",
    "##############################################\n",
    "\n",
    "def custom_clustering(nodes, n_clusters, lambda_clust, max_iter=10):\n",
    "    \"\"\"\n",
    "    Custom clustering that minimizes the objective:\n",
    "      \\(\\sum_{i=1}^{c} \\sum_{j \\in C_i} \\|B_j - \\mu_i\\|_2 + \\lambda_{\\text{clust}} \\cdot |r_j - \\bar{r}_i|\\),\n",
    "    where \\(B_j\\) is the feature vector of node j, \\(r_j\\) is its predicted risk, and \\(\\bar{r}_i\\) is the average risk in cluster i.\n",
    "    Returns a dictionary mapping cluster labels to lists of node indices and updates each node with its cluster assignment.\n",
    "    \"\"\"\n",
    "    # Initialize cluster assignment using KMeans on feature vectors\n",
    "    X = np.array([node['feature_vector'] for node in nodes])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    for i, node in enumerate(nodes):\n",
    "        node['cluster'] = labels[i]\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # Compute centroids and average risk for each cluster\n",
    "        centroids = {}\n",
    "        avg_risks = {}\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for i, node in enumerate(nodes):\n",
    "            clusters[node['cluster']].append(i)\n",
    "        for i in range(n_clusters):\n",
    "            if len(clusters[i]) > 0:\n",
    "                B = np.array([nodes[j]['feature_vector'] for j in clusters[i]])\n",
    "                centroids[i] = np.mean(B, axis=0)\n",
    "                risks = [compute_predicted_risk(nodes[j]) for j in clusters[i]]\n",
    "                avg_risks[i] = np.mean(risks)\n",
    "            else:\n",
    "                centroids[i] = np.zeros_like(nodes[0]['feature_vector'])\n",
    "                avg_risks[i] = 0\n",
    "\n",
    "        changed = False\n",
    "        # Reassign each node to the cluster that minimizes the cost\n",
    "        for i, node in enumerate(nodes):\n",
    "            B_j = np.array(node['feature_vector'])\n",
    "            risk_j = compute_predicted_risk(node)\n",
    "            best_cost = None\n",
    "            best_cluster = None\n",
    "            for c in range(n_clusters):\n",
    "                cost = np.linalg.norm(B_j - centroids[c]) + lambda_clust * abs(risk_j - avg_risks[c])\n",
    "                if best_cost is None or cost < best_cost:\n",
    "                    best_cost = cost\n",
    "                    best_cluster = c\n",
    "            if best_cluster != node['cluster']:\n",
    "                changed = True\n",
    "                node['cluster'] = best_cluster\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    cluster_assignment = {i: [] for i in range(n_clusters)}\n",
    "    for i, node in enumerate(nodes):\n",
    "        cluster_assignment[node['cluster']].append(i)\n",
    "    return cluster_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Cluster-wise Federated Averaging\n",
    "##############################################\n",
    "\n",
    "def cluster_fedavg(nodes, cluster_assignment):\n",
    "    \"\"\"\n",
    "    For each cluster (given by cluster_assignment, a dict mapping cluster label -> list of node indices),\n",
    "    compute the weighted average of the local coefficients (node['local_coef']) weighted by node['num_rows'].\n",
    "    Update each node's global_coef with its cluster’s aggregated beta.\n",
    "    \"\"\"\n",
    "    for clabel, node_ids in cluster_assignment.items():\n",
    "        total_samples = sum(nodes[i]['num_rows'] for i in node_ids)\n",
    "        if total_samples == 0:\n",
    "            continue\n",
    "        weighted_sum = np.zeros_like(nodes[node_ids[0]]['local_coef'])\n",
    "        for i in node_ids:\n",
    "            weighted_sum += nodes[i]['local_coef'] * nodes[i]['num_rows']\n",
    "        cluster_global_coef = weighted_sum / total_samples\n",
    "        for i in node_ids:\n",
    "            nodes[i]['global_coef'] = cluster_global_coef.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Main Simulation Parameters and Loop\n",
    "##############################################\n",
    "\n",
    "def main_simulation():\n",
    "    # Global parameters\n",
    "    num_nodes = 10               # Should match the number of datasets generated.\n",
    "    global_dim = 50              # Total number of features (as in data generation)\n",
    "    max_rounds = 100\n",
    "    T_honest = 10\n",
    "    T_ramp = 5\n",
    "    epsilon_max = 0.05\n",
    "    alpha = 0.1                  # Learning rate for trust update\n",
    "    monte_carlo_runs = 1         # Number of Monte Carlo runs (adjust for testing)\n",
    "    n_clusters = 4               # Number of clusters for feature presence clustering\n",
    "    lambda_clust = 0.6           # Weight for risk difference in clustering\n",
    "\n",
    "    # Designate a set of node IDs as \"noisy\" (adjust as desired)\n",
    "    noisy_node_ids = {0, 4, 9}\n",
    "\n",
    "    # (Assume datasets have already been generated and are in the folder \"datasets_h5\")\n",
    "    node_files = sorted(glob.glob(os.path.join(\"datasets_h5\", \"dataset_node_*.h5\")))\n",
    "    nodes = []\n",
    "    for i, filepath in enumerate(node_files):\n",
    "        df, node_features, feature_vector, num_rows = load_node_dataset(filepath)\n",
    "        if i in noisy_node_ids:\n",
    "            z = np.random.uniform(-1, 1, size=len(node_features))\n",
    "        else:\n",
    "            z = np.zeros(len(node_features))\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'data': df,\n",
    "            'features': node_features,\n",
    "            'feature_vector': feature_vector,   # real-valued vector of length global_dim\n",
    "            'num_rows': num_rows,\n",
    "            'local_coef': np.zeros(global_dim),   # will be computed every round\n",
    "            'global_coef': np.zeros(global_dim),  # updated after federated averaging\n",
    "            'trust': np.ones(num_nodes),          # trust vector for peers (length num_nodes)\n",
    "            'is_noisy': i in noisy_node_ids,\n",
    "            'noise': np.zeros(len(node_features)),  # noise vector for local features\n",
    "            'z': z                              # noise direction vector\n",
    "        })\n",
    "\n",
    "    # Use CuPy for heavy array computations\n",
    "    average_trust_history = cp.zeros((num_nodes, max_rounds))\n",
    "\n",
    "    for mc_run in range(monte_carlo_runs):\n",
    "        print(f\"Running Monte Carlo simulation {mc_run + 1}/{monte_carlo_runs}\")\n",
    "        for node in nodes:\n",
    "            node['trust'] = np.ones(num_nodes)\n",
    "            node['global_coef'] = np.zeros(global_dim)\n",
    "            if node['is_noisy']:\n",
    "                node['noise'] = np.zeros(len(node['features']))\n",
    "        trust_history = cp.zeros((num_nodes, max_rounds))\n",
    "\n",
    "        for round_num in range(max_rounds):\n",
    "            # Determine fraction of data to use; fraction grows with round number\n",
    "            fraction = min(1.0, (round_num + 1) / max_rounds)  # e.g., 1% at round 0, 100% at final round\n",
    "\n",
    "            # --- Node Selection Based on Trust (using CuPy) ---\n",
    "            trust_matrix = cp.array([node['trust'] for node in nodes])\n",
    "            avg_trusts = cp.array([cp.mean(cp.delete(trust_matrix[:, j], j)) for j in range(num_nodes)])\n",
    "            probabilities = avg_trusts / cp.sum(avg_trusts)\n",
    "            probabilities = cp.asnumpy(probabilities)\n",
    "            selected_node_ids = np.random.choice(range(num_nodes), size=6, replace=False, p=probabilities)\n",
    "\n",
    "            # --- Local Training (Compute local coefficients) ---\n",
    "            for node in nodes:\n",
    "                df = node['data']\n",
    "                node_feats = node['features']\n",
    "                df_local = df[node_feats + [\"time\", \"death\"]].copy()\n",
    "                # Use only a fraction of the data (the fraction increases with round number)\n",
    "                num_rows_local = int(len(df_local) * fraction)\n",
    "                if num_rows_local < 1:\n",
    "                    num_rows_local = 1\n",
    "                df_local = df_local.iloc[:num_rows_local]\n",
    "                df_local = df_local.fillna(0)\n",
    "                try:\n",
    "                    cox_model = CoxPHFitter()\n",
    "                    cox_model.fit(df_local, duration_col=\"time\", event_col=\"death\", show_progress=False)\n",
    "                except Exception as e:\n",
    "                    print(f\"Node {node['id']} encountered an error in fitting: {e}\")\n",
    "                    continue\n",
    "                local_coef = get_global_coef_from_model(cox_model, node_feats, global_dim=global_dim)\n",
    "                if node['is_noisy']:\n",
    "                    delta_noise = generate_noise(node, round_num, T_honest, T_ramp, epsilon_max)\n",
    "                    node['noise'] += delta_noise\n",
    "                    for idx_local, feat in enumerate(node['features']):\n",
    "                        try:\n",
    "                            global_idx = int(feat.split(\"_\")[1])\n",
    "                        except:\n",
    "                            continue\n",
    "                        local_coef[global_idx] += node['noise'][idx_local]\n",
    "                node['local_coef'] = local_coef.copy()\n",
    "\n",
    "            # --- Custom Clustering Based on Feature Completeness and Risk Alignment ---\n",
    "            cluster_assignment = custom_clustering(nodes, n_clusters, lambda_clust=lambda_clust, max_iter=10)\n",
    "\n",
    "            # --- Trust Update Within Clusters ---\n",
    "            for clabel, node_ids in cluster_assignment.items():\n",
    "                for i in node_ids:\n",
    "                    if i not in selected_node_ids:\n",
    "                        continue\n",
    "                    node = nodes[i]\n",
    "                    df_local = node['data']\n",
    "                    node_feats = node['features']\n",
    "                    orig_cindex = evaluate_cindex(node['local_coef'], df_local, node_feats, global_dim=global_dim)\n",
    "                    for peer_id in node_ids:\n",
    "                        if peer_id == i:\n",
    "                            continue\n",
    "                        if peer_id >= len(node['trust']):\n",
    "                            continue\n",
    "                        peer = nodes[peer_id]\n",
    "                        avg_coef = (node['local_coef'] + peer['local_coef']) / 2.0\n",
    "                        new_cindex = evaluate_cindex(avg_coef, df_local, node_feats, global_dim=global_dim)\n",
    "                        improvement = new_cindex - orig_cindex\n",
    "                        node['trust'][peer_id] = max(0, min(node['trust'][peer_id] + alpha * improvement, 1))\n",
    "\n",
    "            # --- Cluster-wise Federated Averaging ---\n",
    "            cluster_fedavg(nodes, cluster_assignment)\n",
    "\n",
    "            # --- Record Trust History (using CuPy) ---\n",
    "            trust_matrix = cp.array([node['trust'] for node in nodes])\n",
    "            avg_trusts = cp.array([cp.mean(cp.delete(trust_matrix[:, j], j)) for j in range(num_nodes)])\n",
    "            for j in range(num_nodes):\n",
    "                trust_history[j, round_num] = avg_trusts[j]\n",
    "\n",
    "        average_trust_history += trust_history / monte_carlo_runs\n",
    "\n",
    "    # Convert average_trust_history to a NumPy array for plotting\n",
    "    average_trust_history = cp.asnumpy(average_trust_history)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for node_id in range(num_nodes):\n",
    "        label = f\"Node {node_id+1}\" + (\" (Noisy)\" if node_id in noisy_node_ids else \"\")\n",
    "        linestyle = '--' if node_id in noisy_node_ids else '-'\n",
    "        plt.plot(average_trust_history[node_id], label=label, linestyle=linestyle)\n",
    "\n",
    "    phase = T_honest\n",
    "    while phase < max_rounds:\n",
    "        plt.axvspan(phase, phase + T_ramp, color='red', alpha=0.2, label='Noise Phase' if phase == T_honest else None)\n",
    "        next_phase = phase + T_ramp + T_honest\n",
    "        if phase + T_ramp < max_rounds:\n",
    "            plt.axvspan(phase + T_ramp, min(next_phase, max_rounds), color='blue', alpha=0.1, label='No-Noise Phase' if phase == T_honest else None)\n",
    "        phase = next_phase\n",
    "\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.ylabel('Average Trust Score')\n",
    "    plt.title(f'Average Trust Score Evolution with $\\epsilon_{{max}}={epsilon_max}$, $\\alpha={alpha}$ and Custom Clustering ($\\lambda={lambda_clust}$)')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "##############################################\n",
    "# Experiment: Noise Injection Magnitude over Time\n",
    "##############################################\n",
    "\n",
    "def test_noise_injection(T_honest, T_ramp, epsilon_max_values, rounds=200, d=1):\n",
    "    \"\"\"\n",
    "    For a dummy noisy node with a noise direction vector z (of dimension d),\n",
    "    compute the instantaneous noise injection magnitude (n_t) for each round t,\n",
    "    using the generate_noise function. Returns a dictionary mapping each epsilon_max\n",
    "    value to the list of noise magnitudes over time.\n",
    "    \"\"\"\n",
    "    noise_curves = {}\n",
    "    rounds_list = np.arange(rounds)\n",
    "    for eps in epsilon_max_values:\n",
    "        noise_vals = []\n",
    "        dummy_node = {'z': np.ones(d)}  # simple vector of ones\n",
    "        for t in rounds_list:\n",
    "            noise = generate_noise(dummy_node, t, T_honest, T_ramp, eps)\n",
    "            magnitude = np.linalg.norm(noise)  \n",
    "            noise_vals.append(magnitude)\n",
    "        noise_curves[eps] = noise_vals\n",
    "    return rounds_list, noise_curves\n",
    "\n",
    "# Set parameters for the noise injection experiment\n",
    "T_honest_test = 20\n",
    "T_ramp_test = 10\n",
    "epsilon_max_values = [0.05, 0.1, 0.2, 0.4, 0.8]  # five different values\n",
    "rounds_test = 200\n",
    "\n",
    "rounds_list, noise_curves = test_noise_injection(T_honest_test, T_ramp_test, epsilon_max_values, rounds=rounds_test, d=1)\n",
    "\n",
    "output_dir = \"noise_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for eps, curve in noise_curves.items():\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(rounds_list, curve, label=f'epsilon_max = {eps}')\n",
    "    plt.xlabel('Round (t)')\n",
    "    plt.ylabel('Noise magnitude n_t')\n",
    "    plt.title('Noise magnitude over time (T_honest = {}, T_ramp = {})'.format(T_honest_test, T_ramp_test))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    filename = os.path.join(output_dir, f\"noise_plot_epsilon_{eps}.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved noise plot for epsilon_max = {eps} to {filename}\")\n",
    "\n",
    "##############################################\n",
    "# Main\n",
    "##############################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_simulation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

